# Домашнее задание

## Установка кластера высокой доступности на примере pg_auto_failover 
< https://github.com/citusdata/pg_auto_failover >

## 1. Выполним подготовку инфраструктуры

### 1.1 Создадим ВМ в GCP в конфигурации e2-medium (2 CPU, 4GB RAM), 90 GB SDD.
```
C:\Users\User\AppData\Local\Google\Cloud SDK>gcloud compute instances create instance-15 --project=pgdba-340419 --zone=us-central1-c --machine-type=e2-medium --network-interface=network-tier=PREMIUM,subnet=default --maintenance-policy=MIGRATE --service-account=13258540026-compute@developer.gserviceaccount.com --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append --tags=http-server,https-server --create-disk=auto-delete=yes,boot=yes,device-name=instance-15,image=projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20220204,mode=rw,size=90,type=projects/pgdba-340419/zones/us-central1-c/diskTypes/pd-ssd --no-shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --reservation-affinity=any
Created [https://www.googleapis.com/compute/v1/projects/pgdba-340419/zones/us-central1-c/instances/instance-15].
WARNING: Some requests generated warnings:
 - Disk size: '90 GB' is larger than image size: '10 GB'. You might need to resize the root repartition manually if the operating system does not support automatic resizing. See https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd for details.

NAME         ZONE           MACHINE_TYPE  PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP   STATUS
instance-15  us-central1-c  e2-medium                  10.128.0.2   34.72.200.34  RUNNING
```
### 1.2 Выполним установку pg_auto_failover из исходных кодов
#### 1.2.1 Клонируем репозиторий

```
git clone https://github.com/citusdata/pg_auto_failover.git
```
#### 1.2.2 Установим требуемые инструменты для сборки и зависимости
```
$ sudo apt -y install build-essential postgresql-server-dev-14 libssl-dev libkrb5-dev libncurses6
```
#### 1.2.3 Выполним сборку
```
User@instance-15:~/hw$ cd pg_auto_failover/
User@instance-15:~/hw/pg_auto_failover$ make
make -C src/monitor/ all
make[1]: Entering directory '/home/User/hw/pg_auto_failover/src/monitor'
gcc -std=c99 -D_GNU_SOURCE -g -Wall -Wmissing-prototypes -Wpointer-arith -Wdecla                                                                  ration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wi                                                                  mplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasin                                                                  g -fwrapv -fexcess-precision=standard -Wno-format-truncation -Wno-stringop-trunc                                                                  ation -g -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -fno-o                                                                  mit-frame-pointer -Wformat -Wall -Werror=implicit-int -Werror=implicit-function-                                                                  declaration -Werror=return-type -Wno-declaration-after-statement -Wno-missing-br                                                                  aces  -fPIC -std=c99 -Wall -Werror -Wno-unused-parameter -Iinclude -I/usr/includ                                                                  e/postgresql -g -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/post                                                                  gresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/li                                                                  bxml2   -c -o metadata.o metadata.c

........
/usr/bin/ld: cannot find -lselinux
/usr/bin/ld: cannot find -llz4
/usr/bin/ld: cannot find -lxslt
/usr/bin/ld: cannot find -lxml2
/usr/bin/ld: cannot find -lpam
/usr/bin/ld: cannot find -lz
/usr/bin/ld: cannot find -lreadline

```
### 1.2.4 Установим недостающие библиотеки и повторим сборку
```
$ sudo apt install libreadline-dev libz-dev libpam-dev libxml2-dev libxslt-dev liblz4-dev libselinux-dev
$  make
make -C src/monitor/ all
make[1]: Entering directory '/home/User/hw/pg_auto_failover/src/monitor'
make[1]: Nothing to be done for 'all'.
make[1]: Leaving directory '/home/User/hw/pg_auto_failover/src/monitor'
make -C src/bin/ all
make[1]: Entering directory '/home/User/hw/pg_auto_failover/src/bin'
make -C pg_autoctl pg_autoctl
...
make[2]: Leaving directory '/home/User/hw/pg_auto_failover/src/bin/pg_autoctl'
make[1]: Leaving directory '/home/User/hw/pg_auto_failover/src/bin'

```
#### 1.2.5 Выполним установку
```
User@instance-15:~/hw/pg_auto_failover$ sudo make install -j10
make -C src/monitor/ all
make -C src/bin/ all
make[1]: Entering directory '/home/User/hw/pg_auto_failover/src/monitor'
...
/usr/bin/install -c -m 644 group_state_machine.bc '/usr/lib/postgresql/14/lib/bitcode'/pgautofailover/./
cd '/usr/lib/postgresql/14/lib/bitcode' && /usr/lib/llvm-9/bin/llvm-lto -thinlto -thinlto-action=thinlink -o pgautofailover.index.bc pgautofailover/node_metadata.bc pgautofailover/formation_metadata.bc pgautofailover/node_active_protocol.bc pgautofailover/health_check_worker.bc pgautofailover/pg_auto_failover.bc pgautofailover/notifications.bc pgautofailover/metadata.bc pgautofailover/conninfo.bc pgautofailover/replication_state.bc pgautofailover/health_check_metadata.bc pgautofailover/version_compat.bc pgautofailover/group_state_machine.bc
make[1]: Leaving directory '/home/User/hw/pg_auto_failover/src/monitor'

```
#### 1.2.6 Проверим корректность установки
```
User@instance-15:~/hw/pg_auto_failover$ pg_autoctl --version
pg_autoctl version 1.6.3
pg_autoctl extension version 1.6
compiled with PostgreSQL 14.0 (Ubuntu 14.0-1.pgdg20.04+1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0, 64-bit
compatible with Postgres 10, 11, 12, 13, and 14
```

### 2. Настроим работу кластера высокой доступности
### 2.1 Запустим монитор состояния кластера
```
User@instance-15:~/hw$ sudo chown -R postgres:postgres pg_auto_failover/
User@instance-15:~/hw/pg_auto_failover/src$ sudo su postgres
$ export PATH="$PATH:/usr/lib/postgresql/14/bin/"
$ export PGDATA=./monitor
$ export PGPORT=5000
$ pg_autoctl create monitor --ssl-self-signed --hostname localhost --auth trust --run
postgres@instance-15:/home/User/hw/pg_auto_failover/src$ pg_autoctl create monitor --ssl-self-signed --hostname localhost --auth trust --run
12:29:14 10870 INFO  Using default --ssl-mode "require"
12:29:14 10870 INFO  Using --ssl-self-signed: pg_autoctl will create self-signed certificates, allowing for encrypted network traffic
12:29:14 10870 WARN  Self-signed certificates provide protection against eavesdropping; this setup does NOT protect against Man-In-The-Middle attacks nor Impersonation attacks.
12:29:14 10870 WARN  See https://www.postgresql.org/docs/current/libpq-ssl.html for details
12:29:14 10870 INFO  Initialising a PostgreSQL cluster at "./monitor"
12:29:14 10870 INFO  /usr/lib/postgresql/14/bin/pg_ctl initdb -s -D ./monitor --option '--auth=trust'
12:29:15 10870 INFO   /usr/bin/openssl req -new -x509 -days 365 -nodes -text -out /home/User/hw/pg_auto_failover/src/monitor/server.crt -keyout /home/User/hw/pg_auto_failover/src/monitor/server.key -subj "/CN=localhost"
12:29:15 10870 INFO  Started pg_autoctl postgres service with pid 10890
12:29:15 10890 INFO   /usr/bin/pg_autoctl do service postgres --pgdata ./monitor -v
12:29:15 10870 INFO  Started pg_autoctl listener service with pid 10891
12:29:15 10896 INFO   /usr/lib/postgresql/14/bin/postgres -D /home/User/hw/pg_auto_failover/src/monitor -p 5000 -h *
12:29:15 10890 INFO  Postgres is now serving PGDATA "/home/User/hw/pg_auto_failover/src/monitor" on port 5000 with pid 10896
12:29:15 10891 WARN  NOTICE:  installing required extension "btree_gist"
12:29:15 10891 INFO  Granting connection privileges on 127.0.0.0/8
12:29:15 10891 WARN  Skipping HBA edits (per --skip-pg-hba) for rule: hostssl "pg_auto_failover" "autoctl_node" 127.0.0.0/8 trust
12:29:15 10891 INFO  Your pg_auto_failover monitor instance is now ready on port 5000.
12:29:15 10891 INFO  Monitor has been successfully initialized.
12:29:15 10891 INFO   /usr/bin/pg_autoctl do service listener --pgdata ./monitor -v
12:29:15 10891 INFO  Managing the monitor at postgres://autoctl_node@localhost:5000/pg_auto_failover?sslmode=require
12:29:15 10891 INFO  Reloaded the new configuration from "/var/lib/postgresql/.config/pg_autoctl/home/User/hw/pg_auto_failover/src/monitor/pg_autoctl.cfg"
12:29:15 10891 INFO  Reloading Postgres configuration and HBA rules
12:29:17 10891 INFO  The version of extension "pgautofailover" is "1.6" on the monitor
12:29:17 10891 INFO  Contacting the monitor to LISTEN to its events.

```
# 2.2 Получим строку подключения к монитору из параллельной сессии
```
postgres@instance-15:/home/User$ pg_autoctl show uri  ./monitor                         
	Type |    Name | Connection String
-------------+---------+-------------------------------
     monitor | monitor | postgres://autoctl_node@localhost:5000/pg_auto_failover?sslmode=require
   formation | default |
```

### 2.3 Создадим небольшой диск для работы мастер-кластера
```
User@instance-15:~/hw/pg_auto_failover$ sudo mkdir /mnt/node_a
User@instance-15:~/hw/pg_auto_failover$ sudo mount -t tmpfs -o size=400m tmpfs /mnt/node_a
User@instance-15:~/hw/pg_auto_failover$ sudo mkdir /mnt/node_a/data
User@instance-15:~/hw/pg_auto_failover$ sudo chown postgres -R /mnt/node_a

```
### 2.3 Выполним инициализацию Postgres на созданном диске
```
$ sudo mkdir /mnt/backup
$ sudo chown postgres:postgres /mnt/backup
$ export PGDATA=/mnt/node_a
$ export PGPORT=5001
pg_autoctl create postgres \
    --hostname localhost \
    --auth trust \
    --ssl-self-signed \
    --monitor 'postgres://autoctl_node@localhost:5000/pg_auto_failover?sslmode=require' \
    --run

12:46:02 11253 INFO  Using default --ssl-mode "require"
12:46:02 11253 INFO  Using --ssl-self-signed: pg_autoctl will create self-signed certificates, allowing for encrypted network traffic
12:46:02 11253 WARN  Self-signed certificates provide protection against eavesdropping; this setup does NOT protect against Man-In-The-Middle attacks nor Impersonation attacks.
12:46:02 11253 WARN  See https://www.postgresql.org/docs/current/libpq-ssl.html for details
12:46:02 11253 INFO  Getting nodes from the monitor for group 0 in formation "default"
12:46:02 11253 INFO  Node name on the monitor is now "node_19"
12:46:02 11253 INFO  Started pg_autoctl postgres service with pid 11256
12:46:02 11253 INFO  Started pg_autoctl node-active service with pid 11257
12:46:02 11257 INFO  Continuing from a previous `pg_autoctl create` failed attempt
12:46:02 11257 INFO  PostgreSQL state at registration time was: PGDATA does not exist
12:46:02 11257 INFO  FSM transition from "init" to "single": Start as a single node
12:46:02 11257 INFO  Initialising postgres as a primary
12:46:02 11257 INFO  Initialising a PostgreSQL cluster at "/mnt/node_a"
12:46:02 11257 INFO  /usr/lib/postgresql/14/bin/pg_ctl initdb -s -D /mnt/node_a --option '--auth=trust'
12:46:02 11256 INFO   /usr/bin/pg_autoctl do service postgres --pgdata /mnt/node_a -v
12:46:03 11257 INFO   /usr/bin/openssl req -new -x509 -days 365 -nodes -text -out /mnt/node_a/server.crt -keyout /mnt/node_a/server.key -subj "/CN=localhost"
12:46:03 11283 INFO   /usr/lib/postgresql/14/bin/postgres -D /mnt/node_a -p 5001 -h *
12:46:03 11257 INFO  The user "postgres" already exists, skipping.
12:46:03 11257 INFO  CREATE DATABASE postgres;
12:46:03 11257 INFO  The database "postgres" already exists, skipping.
12:46:03 11257 INFO  CREATE EXTENSION pg_stat_statements;
12:46:03 11256 INFO  Postgres is now serving PGDATA "/mnt/node_a" on port 5001 with pid 11283
12:46:03 11257 INFO  Disabling synchronous replication
12:46:03 11257 INFO  Reloading Postgres configuration and HBA rules
12:46:03 11257 INFO   /usr/bin/openssl req -new -x509 -days 365 -nodes -text -out /mnt/node_a/server.crt -keyout /mnt/node_a/server.key -subj "/CN=localhost"
12:46:04 11257 INFO  Contents of "/mnt/node_a/postgresql-auto-failover.conf" have changed, overwriting
12:46:04 11257 INFO  Reloading Postgres configuration and HBA rules
12:46:04 11257 INFO  Transition complete: current state is now "single"
12:46:04 11257 INFO  keeper has been successfully initialized.
12:46:04 11257 INFO   /usr/bin/pg_autoctl do service node-active --pgdata /mnt/node_a -v
12:46:04 11257 INFO  Reloaded the new configuration from "/var/lib/postgresql/.config/pg_autoctl/mnt/node_a/pg_autoctl.cfg"
12:46:04 11257 INFO  Reloading Postgres configuration and HBA rules
12:46:04 11257 INFO  pg_autoctl service is running, current state is "single"
12:46:04 11257 INFO  New state for this node (node 19, "node_19") (localhost:5001): single ➜ single

```
### 2.4 Инициализируем дополнительный экземпляр PostgreSQL
```
$ sudo su postgres

$ export PATH="$PATH:/usr/lib/postgresql/14/bin/"
$ export PGDATA=./node_b
$ export PGPORT=5002
$ pg_autoctl create postgres \
    --hostname localhost \
    --auth trust \
    --ssl-self-signed \
    --monitor 'postgres://autoctl_node@localhost:5000/pg_auto_failover?sslmode=require' \
    --run
12:55:36 14749 INFO  Using default --ssl-mode "require"
12:55:36 14749 INFO  Using --ssl-self-signed: pg_autoctl will create self-signed certificates, allowing for encrypted network traffic
12:55:36 14749 WARN  Self-signed certificates provide protection against eavesdropping; this setup does NOT protect against Man-In-The-Middle attacks nor Impersonation attacks.
12:55:36 14749 WARN  See https://www.postgresql.org/docs/current/libpq-ssl.html for details
12:55:36 14749 INFO  Started pg_autoctl postgres service with pid 14751
12:55:36 14751 INFO   /usr/bin/pg_autoctl do service postgres --pgdata ./node_b -v
12:55:36 14749 INFO  Started pg_autoctl node-active service with pid 14753
12:55:36 14753 INFO  Registered node 20 "node_20" (localhost:5002)in formation "default", group 0, state "wait_standby"
12:55:36 14753 INFO  Writing keeper state file at "/var/lib/postgresql/.local/share/pg_autoctl/var/lib/postgresql/node_b/pg_autoctl.state"
12:55:36 14753 INFO  Writing keeper init state file at "/var/lib/postgresql/.local/share/pg_autoctl/var/lib/postgresql/node_b/pg_autoctl.init"
12:55:36 14753 INFO  Successfully registered as "wait_standby" to the monitor.
12:55:36 14753 INFO  FSM transition from "init" to "wait_standby": Start following a primary
12:55:36 14753 INFO  Transition complete: current state is now "wait_standby"
12:55:36 14753 INFO  New state for node 19 "node_19" (localhost:5001): single ➜ wait_primary
12:55:36 14753 INFO  New state for node 19 "node_19" (localhost:5001): wait_primary ➜ wait_primary
12:55:36 14753 INFO  FSM transition from "wait_standby" to "catchingup": The primary is now ready to accept a standby
12:55:36 14753 INFO  Initialising PostgreSQL as a hot standby
12:55:36 14753 INFO   /usr/lib/postgresql/14/bin/pg_basebackup -w -d 'application_name=pgautofailover_standby_20 host=localhost port=5001 user=pgautofailover_replicator sslmode=require' --pgdata /var/lib/postgresql/backup/node_20 -U pgautofailover_replicator --verbose --progress --max-rate 100M --wal-method=stream --slot pgautofailover_standby_20
12:55:36 14753 INFO  pg_basebackup:
12:55:36 14753 INFO
12:55:36 14753 INFO  initiating base backup, waiting for checkpoint to complete
12:55:36 14753 INFO  pg_basebackup:
12:55:36 14753 INFO
12:55:36 14753 INFO  checkpoint completed
12:55:36 14753 INFO  pg_basebackup:
12:55:36 14753 INFO
12:55:36 14753 INFO  write-ahead log start point: 0/2000028 on timeline 1
12:55:36 14753 INFO  pg_basebackup:
12:55:36 14753 INFO
12:55:36 14753 INFO  starting background WAL receiver
12:55:36 14753 INFO      0/26303 kB (0%), 0/1 tablespace (...esql/backup/node_20/backup_label)
12:55:37 14753 INFO   8600/26303 kB (32%), 0/1 tablespace (...kup/node_20/base/13726/13554_fsm)
12:55:37 14753 INFO  26312/26312 kB (100%), 0/1 tablespace (...backup/node_20/global/pg_control)
12:55:37 14753 INFO  26312/26312 kB (100%), 1/1 tablespace                      
12:55:37 14753 INFO  pg_basebackup:
12:55:37 14753 INFO
12:55:37 14753 INFO  write-ahead log end point: 0/2000138
12:55:37 14753 INFO  pg_basebackup:
12:55:37 14753 INFO
12:55:37 14753 INFO  waiting for background process to finish streaming ...
12:55:37 14753 INFO  pg_basebackup:
12:55:37 14753 INFO
12:55:37 14753 INFO  syncing data to disk ...
12:55:37 14753 INFO  pg_basebackup:
12:55:37 14753 INFO
12:55:37 14753 INFO  renaming backup_manifest.tmp to backup_manifest
12:55:37 14753 INFO  pg_basebackup:
12:55:37 14753 INFO
12:55:37 14753 INFO  base backup completed
12:55:37 14753 INFO  Creating the standby signal file at "/var/lib/postgresql/node_b/standby.signal", and replication setup at "/var/lib/postgresql/node_b/postgresql-auto-failover-standby.conf"
12:55:37 14753 INFO   /usr/bin/openssl req -new -x509 -days 365 -nodes -text -out /var/lib/postgresql/node_b/server.crt -keyout /var/lib/postgresql/node_b/server.key -subj "/CN=localhost"
12:55:37 14753 INFO  Contents of "/var/lib/postgresql/node_b/postgresql-auto-failover.conf" have changed, overwriting
12:55:37 14786 INFO   /usr/lib/postgresql/14/bin/postgres -D /var/lib/postgresql/node_b -p 5002 -h *
12:55:38 14753 INFO  PostgreSQL started on port 5002
12:55:38 14753 INFO  Fetched current list of 1 other nodes from the monitor to update HBA rules, including 1 changes.
12:55:38 14753 INFO  Ensuring HBA rules for node 19 "node_19" (localhost:5001)
12:55:38 14753 INFO  Reloading Postgres configuration and HBA rules
12:55:38 14753 INFO  Transition complete: current state is now "catchingup"
12:55:38 14751 INFO  Postgres is now serving PGDATA "/var/lib/postgresql/node_b" on port 5002 with pid 14786
12:55:38 14753 INFO  keeper has been successfully initialized.
12:55:38 14753 INFO   /usr/bin/pg_autoctl do service node-active --pgdata ./node_b -v
12:55:38 14753 INFO  Reloaded the new configuration from "/var/lib/postgresql/.config/pg_autoctl/var/lib/postgresql/node_b/pg_autoctl.cfg"
12:55:38 14753 INFO  Reloading Postgres configuration and HBA rules
12:55:38 14753 INFO  pg_autoctl service is running, current state is "catchingup"
12:55:38 14753 INFO  Fetched current list of 1 other nodes from the monitor to update HBA rules, including 1 changes.
12:55:38 14753 INFO  Ensuring HBA rules for node 19 "node_19" (localhost:5001)
12:55:38 14753 INFO  Reloading Postgres configuration and HBA rules
12:55:39 14753 INFO  New state for this node (node 20, "node_20") (localhost:5002): catchingup ➜ catchingup
12:55:39 14753 INFO  Monitor assigned new state "secondary"
12:55:39 14753 INFO  FSM transition from "catchingup" to "secondary": Convinced the monitor that I'm up and running, and eligible for promotion again
12:55:39 14753 INFO  Reached timeline 1, same as upstream node 19 "node_19" (localhost:5001)
12:55:39 14753 INFO  Creating replication slot "pgautofailover_standby_19"
12:55:39 14753 INFO  Transition complete: current state is now "secondary"
12:55:39 14753 INFO  New state for this node (node 20, "node_20") (localhost:5002): catchingup ➜ secondary
12:55:39 14753 INFO  New state for this node (node 20, "node_20") (localhost:5002): secondary ➜ secondary
12:55:39 14753 INFO  New state for node 19 "node_19" (localhost:5001): wait_primary ➜ primary
12:55:40 14753 INFO  New state for node 19 "node_19" (localhost:5001): primary ➜ primary

```
### 2.5 Проверим статус кластера
```
postgres@instance-15:/home/User$ pg_autoctl show state --monitor=postgres://autoctl_node@localhost:5000/pg_auto_failover
   Name |  Node |      Host:Port |       TLI: LSN |   Connection |      Reported State |      Assigned State
--------+-------+----------------+----------------+--------------+---------------------+--------------------
node_19 |    19 | localhost:5001 |   1: 0/30001F8 |   read-write |             primary |             primary
node_20 |    20 | localhost:5002 |   1: 0/30001F8 |    read-only |           secondary |           secondary
```


### 2.6 Проверим корректность работы с данными
### 2.6.1 Создадим таблицу на мастер-экземпляре и наполним ее данными
```sql
postgres@instance-15:/home/User$ psql -p 5001 -c 'create table test as select generate_series(1,1000000) values;'
SELECT 1000000

```
### 2.6.2 Выполним выборку на реплике
```sql
postgres@instance-15:/home/User$ psql -p 5002 -c 'select count(*) from test;'
  count
---------
 1000000
(1 row)
```

### 3. Проведем испытания HA-кластера
### 3.1 Инициируем switchover
```console
postgres@instance-15:/home/User$ pg_autoctl perform switchover  --monitor=postgres://autoctl_node@localhost:5000/pg_auto_failover
13:15:23 29973 INFO  Waiting 60 secs for a notification with state "primary" in formation "default" and group 0
13:15:23 29973 INFO  Listening monitor notifications about state changes in formation "default" and group 0
13:15:23 29973 INFO  Following table displays times when notifications are received
    Time |    Name |  Node |      Host:Port |       Current State |      Assigned State
---------+---------+-------+----------------+---------------------+--------------------
13:15:23 | node_19 |    19 | localhost:5001 |             primary |            draining
13:15:23 | node_20 |    20 | localhost:5002 |           secondary |   prepare_promotion
13:15:23 | node_20 |    20 | localhost:5002 |   prepare_promotion |   prepare_promotion
13:15:23 | node_20 |    20 | localhost:5002 |   prepare_promotion |    stop_replication
13:15:23 | node_19 |    19 | localhost:5001 |             primary |      demote_timeout
13:15:23 | node_19 |    19 | localhost:5001 |            draining |      demote_timeout
13:15:23 | node_19 |    19 | localhost:5001 |      demote_timeout |      demote_timeout
13:15:24 | node_20 |    20 | localhost:5002 |    stop_replication |    stop_replication
13:15:24 | node_20 |    20 | localhost:5002 |    stop_replication |        wait_primary
13:15:24 | node_19 |    19 | localhost:5001 |      demote_timeout |             demoted
13:15:24 | node_19 |    19 | localhost:5001 |             demoted |             demoted
13:15:24 | node_20 |    20 | localhost:5002 |        wait_primary |        wait_primary
13:15:24 | node_19 |    19 | localhost:5001 |             demoted |          catchingup
13:15:26 | node_19 |    19 | localhost:5001 |          catchingup |          catchingup
13:15:27 | node_19 |    19 | localhost:5001 |          catchingup |           secondary
13:15:27 | node_19 |    19 | localhost:5001 |           secondary |           secondary
13:15:27 | node_20 |    20 | localhost:5002 |        wait_primary |             primary
13:15:27 | node_20 |    20 | localhost:5002 |             primary |             primary

```
### 3.2 Проверим статус кластера и убедимся в успешном переключении
```
postgres@instance-15:/home/User$ pg_autoctl show state --monitor=postgres://autoctl_node@localhost:5000/pg_auto_failover
   Name |  Node |      Host:Port |       TLI: LSN |   Connection |      Reported State |      Assigned State
--------+-------+----------------+----------------+--------------+---------------------+--------------------
node_19 |    19 | localhost:5001 |   2: 0/6D9D9B0 |    read-only |           secondary |           secondary
node_20 |    20 | localhost:5002 |   2: 0/6D9D9B0 |   read-write |             primary |             primary

```
**Вывод**: Развернут кластер высокой доступности из двух экземпляров PostgreSql c использованием потоковой репликации на базе pg_auto_failover

## 4. Материалы
<https://pg-auto-failover.readthedocs.io/en/master/>
<https://cloudblogs.microsoft.com/opensource/2019/05/06/introducing-pg_auto_failover-postgresql-open-source-extension-automated-failover-high-availability/>
